%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
%              (C) Copyright 1995 The Board of Trustees of the             %
%                          University of Illinois                          %
%                           All Rights Reserved                            %
%								  	   %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Running \NAMD}
\label{section:run}

NAMD runs on a variety of serial and parallel platforms.  While it is
trivial to launch a serial program, a parallel program depends on a
platform-specific library such as MPI to launch copies of itself on
other nodes and to provide access to a high performance network such as 
% Myrinet or 
InfiniBand if one is available.

For typical workstations (Windows, Linux, Mac OS X, or other Unix)
with only ethernet networking (hopefully gigabit), NAMD uses the
Charm++ native communications layer and the program charmrun to launch
namd3 processes for parallel runs (either exclusively on the local
machine with the ++local option or on other hosts as specified by a
nodelist file).  The namd3 binaries for these platforms can also be
run directly (known as standalone mode) for single process runs.

\subsection{Individual Windows, Linux, Mac OS X, or Other Unix Workstations}

Individual workstations use the same version of NAMD as workstation
networks, but running NAMD is much easier.  If your machine has only
one processor core you can run any netlrts or multicore namd3 binary
directly:

\begin{verbatim}
  namd3 <configfile>
\end{verbatim}

% Windows, 
Mac OX X (Intel) and Linux-x86\_64-multicore released binaries
are based on ``multicore'' builds of Charm++ that can run multiple threads.
These multicore builds lack a network layer, so they can only be used on a
single machine.
For best performance use one thread per processor with the +p option:

\begin{verbatim}
  namd3 +p<procs> <configfile>
\end{verbatim}

Windows native builds are no longer provided for download. 
Instead, the Windows platform is now supported by running 
Linux-x86\_64-multicore builds using WSL2 on Windows 10 or 11. 
Using the Linux builds through WSL2 generally offers higher performance 
than running native builds. 

For other multiprocessor workstations the included charmrun program is
needed to run multiple namd3 processes.  The ++local option is also
required to specify that only the local machine is being used:
\begin{verbatim}
  charmrun namd3 ++local +p<procs> <configfile>
\end{verbatim}
You may need to specify the full path to the namd3 binary.

% \subsection{Windows Clusters and Workstation Networks}
% 
% The Win64-MPI version of NAMD runs on Windows HPC Server and should be
% launched as you would any other MPI program.

\subsection{Linux Clusters with InfiniBand or Other High-Performance Networks}

Charm++ provides a special verbs network layer that uses InfiniBand
networks directly through the OpenFabrics OFED ibverbs library.  This
avoids efficiency and portability issues associated with MPI.  
% Look for
% pre-built ibverbs NAMD binaries or specify ibverbs when building Charm++.
This newer Charm++ verbs network layer replaces the earlier 
ibverbs layer, providing equivalent performance 
while also supporting multi-copy algorithms (replicas).
  
Intel Omni-Path networks are incompatible with the pre-built verbs
NAMD binaries.  Charm++ for verbs can be built with --with-qlogic 
to support Omni-Path, but for this case the Charm++ MPI network layer performs
better than the verbs layer.  Hangs have been observed with Intel MPI
but not with OpenMPI, so OpenMPI is preferred.  See ``Compiling NAMD''
below for MPI build instructions.  NAMD MPI binaries may be launched
directly with mpiexec rather than via the provided charmrun script.

Writing batch job scripts to run charmrun in a queueing system can be
challenging.  Since most clusters provide directions for using mpiexec
to launch MPI jobs, charmrun provides a ++mpiexec option to use mpiexec
to launch non-MPI binaries.  If ``mpiexec -n {\em procs} ...'' is not
sufficient to launch jobs on your cluster you will need to write an
executable mympiexec script like the following from TACC:

\begin{verbatim}
  #!/bin/csh
  shift; shift; exec ibrun $*
\end{verbatim}

The job is then launched (with full paths where needed) as:

\begin{verbatim}
  charmrun +p<procs> ++mpiexec ++remote-shell mympiexec namd3 <configfile>
\end{verbatim}

Charm++ now provides the option ++mpiexec-no-n for the common case
where mpiexec does not accept "-n {\em procs}" and instead derives the
number of processes to launch directly from the queueing system:

\begin{verbatim}
  charmrun +p<procs> ++mpiexec-no-n ++remote-shell ibrun namd3 <configfile>
\end{verbatim}

For workstation clusters and other massively parallel machines with
special high-performance networking, NAMD uses the system-provided MPI
library (with a few exceptions) and standard system tools such as mpirun
are used to launch jobs.  Since MPI libraries are very often incompatible
between versions, you will likely need to recompile NAMD and its
underlying Charm++ libraries to use these machines in parallel (the
provided non-MPI binaries should still work for serial runs.) The provided
charmrun program for these platforms is only a script that attempts to
translate charmrun options into mpirun options, but due to the diversity
of MPI libraries it often fails to work.

\subsection{Linux or Other Unix Workstation Networks}

The same binaries used for individual workstations as described above
(other than pure ``multicore'' builds and MPI builds)
can be used with charmrun to run in parallel on a workstation network.
The only difference is that you must provide a ``nodelist'' file listing
the machines where namd3 processes should run, for example:

\begin{verbatim}
  group main
  host brutus
  host romeo
\end{verbatim}

The ``group main'' line defines the default machine list.  Hosts brutus
and romeo are the two machines on which to run the simulation.  Note
that charmrun may run on one of those machines, or charmrun may run
on a third machine.  All machines used for a simulation must be of the
same type and have access to the same namd3 binary.

By default, the ``rsh'' command is used to start namd3
on each node specified in the nodelist file.  You can change this via
the CONV\_RSH environment variable, i.e., to use ssh instead of rsh run
``setenv CONV\_RSH ssh'' or add it to your login or batch script.  You
must be able to connect to each node via rsh/ssh without typing your
password; this can be accomplished via a .rhosts files in your home
directory, by an /etc/hosts.equiv file installed by your sysadmin, or
by a .ssh/authorized\_keys file in your home directory.  You should
confirm that you can run ``ssh hostname pwd'' (or ``rsh hostname pwd'')
without typing a password before running NAMD.  Contact your local
sysadmin if you have difficulty setting this up.  If you are unable to
use rsh or ssh, then add ``setenv CONV\_DAEMON'' to your script and run 
charmd (or charmd\_faceless, which produces a log file) on every node.

You should now be able to try running NAMD as:

\begin{verbatim}
  charmrun namd3 +p<procs> <configfile>
\end{verbatim}

If this fails or just hangs, try adding the ++verbose option to see
more details of the startup process.  You may need to specify the full
path to the namd3 binary.  Charmrun will start the number of processes
specified by the +p option, cycling through the hosts in the nodelist
file as many times as necessary.  You may list multiprocessor machines
multiple times in the nodelist file, once for each processor.

You may specify the nodelist file with the ``++nodelist'' option and the
group (which defaults to ``main'') with the ``++nodegroup'' option.  If
you do not use ``++nodelist'' charmrun will first look for ``nodelist''
in your current directory and then ``.nodelist'' in your home directory.

Some automounters use a temporary mount directory which is prepended
to the path returned by the pwd command.  To run on multiple machines
you must add a ``++pathfix'' option to your nodelist file.  For example:

\begin{verbatim}
  group main ++pathfix /tmp\_mnt /
  host alpha1
  host alpha2
\end{verbatim}

There are many other options to charmrun and for the nodelist file.
These are documented at in the Charm++ Installation and Usage Manual
available at \url{http://charm.cs.uiuc.edu/manuals/} and a list of available
charmrun options is available by running charmrun without arguments.

If your workstation cluster is controlled by a queueing system you
will need build a nodelist file in your job script.  For example, if
your queueing system provides a HOST\_FILE environment variable:

\begin{verbatim}
  set NODES = `cat $HOST_FILE`
  set NODELIST = $TMPDIR/namd3.nodelist
  echo group main >! $NODELIST
  foreach node ( $NODES )
    echo host $node >> $NODELIST
  end
  @ NUMPROCS = 2 * $#NODES
  charmrun namd3 +p$NUMPROCS ++nodelist $NODELIST <configfile>
\end{verbatim}

Note that NUMPROCS is twice the number of nodes in this example.
This is the case for dual-processor machines.  For single-processor
machines you would not multiply \$\#NODES by two.

Note that these example scripts and the setenv command are for the csh
or tcsh shells.  They must be translated to work with sh or bash.

\subsection{Shared-Memory and Network-Based Parallelism (SMP Builds)}

The Linux-x86\_64-verbs-smp and Linux-x86\_64-netlrts-smp released binaries are
based on ``smp'' builds of Charm++ that can be used with multiple threads
on either a single machine like a multicore build, or across a network.
SMP builds combine multiple worker threads and an extra communication
thread into a single process.  Since one core per process is used for
the communication thread SMP builds might be slower than non-SMP
builds.  The advantage of SMP builds is that many data structures are
shared among the threads, reducing the per-core memory footprint when
scaling large simulations to large numbers of cores.

SMP builds launched with charmrun use ++n to specify the total number of
processes (Charm++ "nodes") and ++ppn to specify the number of PEs (Charm++
worker threads) per process.  Prevous versions required the use of +p to
specify the total number of PEs, but the new ++n option is now recommended.
Thus, to run one process with one communication and three worker threads
on each of four quad-core nodes one would specify:

\begin{verbatim}
  charmrun namd3 ++n 4 ++ppn 3 <configfile>
\end{verbatim}

For MPI-based SMP builds one would specify any mpiexec options needed
for the required number of processes and pass +ppn to the NAMD binary as:

\begin{verbatim}
  mpiexec -n 4 namd3 +ppn 3 <configfile>
\end{verbatim}

See the Cray XE/XK/XC directions below for a more complex example.

\subsection{Cray XE/XK/XC}

First load modules for the GNU compilers (XE/XK only, XC should use Intel),
topology information, huge page sizes, and the system FFTW 3 library:

\begin{verbatim}
  module swap PrgEnv-cray PrgEnv-gnu
  module load rca
  module load craype-hugepages8M
  module load fftw
\end{verbatim}

The CUDA Toolkit module enables dynamic linking, so it should only
be loaded when building CUDA binaries and never for non-CUDA binaries:

\begin{verbatim}
  module load cudatoolkit
\end{verbatim}

For CUDA or large simulations on XE/XK use gemini\_gni-crayxe-persistent-smp
and for smaller XE simulations use gemini\_gni-crayxe-persistent.  For XC
similarly use gni-crayxc-persistent-smp or gni-crayxc-persistent.

For XE/XK use CRAY-XE-gnu and (for CUDA) the ``--with-cuda'' config option,
the appropriate ``--charm-arch'' parameter, and --with-fftw3.  For XC
use instead CRAY-XC-intel but all other options the same.

Your batch job will need to load modules and set environment variables:

\begin{verbatim}
  module swap PrgEnv-cray PrgEnv-gnu
  module load rca
  module load craype-hugepages8M
  setenv HUGETLB_DEFAULT_PAGE_SIZE 8M
  setenv HUGETLB_MORECORE no
\end{verbatim}

To run an SMP build with one process per node on 16 32-core nodes:

\begin{verbatim}
  aprun -n 16 -r 1 -N 1 -d 31 /path/to/namd3 +ppn 30 +pemap 1-30 +commap 0 <configfile>
\end{verbatim}

or the same with 4 processes per node:

\begin{verbatim}
  aprun -n 64 -N 4 -d 8 /path/to/namd3 +ppn 7 \
            +pemap 1-7,9-15,17-23,25-31 +commap 0,8,16,24 <configfile>
\end{verbatim}

or non-SMP, leaving one core free for the operating system:

\begin{verbatim}
  aprun -n 496 -r 1 -N 31 -d 1 /path/to/namd3 +pemap 0-30 <configfile>
\end{verbatim}

The explicit +pemap and +commap settings are necessary to avoid having
multiple threads assigned to the same core (or potentially all threads
assigned to the same core).  If the performance of NAMD running on a
single compute node is much worse than comparable non-Cray host then
it is very likely that your CPU affinity settings need to be fixed.

All Cray XE/XK/XC network layers support multi-copy algorithms (replicas).

\subsection{Xeon Phi Processors (KNL)}

Special Linux-KNL-icc and CRAY-XC-KNL-intel builds enable vectorizable
mixed-precision kernels while preserving full alchemical and other
functionality.  Multi-host runs require multiple smp processes per host
(as many as 13 for Intel Omni-Path, 6 for Cray Aries) in order to drive
the network. Careful attention to CPU affinity settings (see below) is
required, as is 1 or 2 (but not 3 or 4) hyperthreads per PE core (but
only 1 per communication thread core).

There appears to be a bug in the Intel 17.0 compiler that breaks the 
non-KNL-optimized NAMD kernels (used for alchemical free energy, etc.) 
on KNL.  Therefore the Intel 16.0 compilers are recommended on KNL.

\subsection{SGI Altix UV}

Use Linux-x86\_64-multicore and the following script to set CPU affinity:

\begin{verbatim}
  namd3 +setcpuaffinity `numactl --show | awk '/^physcpubind/ {printf \
     "+p%d +pemap %d",(NF-1),$2; for(i=3;i<=NF;++i){printf ",%d",$i}}'` ...
\end{verbatim}

\subsection{IBM POWER Clusters}

Run the verbs or ibverbs version of NAMD as on any other cluster,
using poe in place of mpiexec as the process launcher, for example:
  
\begin{verbatim}
  charmrun +p<procs> ++mpiexec-no-n ++remote-shell poe namd3 <configfile>
\end{verbatim}
  
The details of job submission will vary between sites.  For example,
two nodes with two tasks per node on LSF are -n 4 -R "span[ptile=2]"
with charmrun options +p36 ++ppn 9 ++mpiexec-no-n ++remote-shell poe

\subsection{CPU Affinity}

NAMD may run faster on some machines if threads or processes are set to
run on (or not run on) specific processor cores (or hardware threads).
On Linux this can be done at the process level with the numactl utility,
but NAMD provides its own options for assigning threads to cores.  This
feature is enabled by adding +setcpuaffinity to the namd3 command line,
which by itself will cause NAMD (really the underlying Charm++ library)
to assign threads/processes round-robin to available cores in the order
they are numbered by the operating system.  This may not be the fastest
configuration if NAMD is running fewer threads than there are cores
available and consecutively numbered cores share resources such as
memory bandwidth or are hardware threads on the same physical core.

If needed, specific cores for the Charm++ PEs (processing elements) and
communication threads (on SMP builds) can be set by adding the +pemap
and (if needed) +commap options with lists of core sets in the form
``lower[-upper[:stride[.run]]][,...]''.  A single number identifies a
particular core.  Two numbers separated by a dash identify an inclusive
range (lower bound and upper bound).  If they are followed by a colon and
another number (a stride), that range will be stepped through in increments
of the additional number.  Within each stride, a dot followed by a run will
indicate how many cores to use from that starting point.  For example, the
sequence 0-8:2,16,20-24 includes cores 0, 2, 4, 6, 8, 16, 20, 21, 22, 23, 24.
On a 4-way quad-core system three cores from each socket would be 0-15:4.3
if cores on the same chip are numbered consecutively.  There is no need
to repeat cores for each node in a run as they are reused in order.

For example, the IBM POWER7 has four hardware threads per core and the
first thread can use all of the core's resources if the other threads are
idle; threads 0 and 1 split the core if threads 2 and 3 are idle, but
if either of threads 2 or 3 are active the core is split four ways.  The
fastest configuration of 32 threads or processes on a 128-thread 32-core
is therefore ``+setcpuaffinity +pemap 0-127:4''.  For 64 threads we need
cores 0,1,4,5,8,9,... or 0-127:4.2.  Running 4 processes with +ppn 31
would be ``+setcpuaffinity +pemap 0-127:32.31 +commap 31-127:32''

For Intel processors, including KNL, where hyperthreads on the same core
are not numbered consecutively, hyperthreads may be mapped to consecutive
PEs by appending [+span] to a core set, e.g., ``+pemap 0-63+64+128+192''
to use all threads on a 64-core, 256-thread KNL with threads mapped to
PEs as 0,64,128,192,1,65,129,193,...

For an Altix UV or other machines where the queueing system assigns cores
to jobs this information must be obtained with numactl --show and passed
to NAMD in order to set thread affinity (which will improve performance):

\begin{verbatim}
  namd3 +setcpuaffinity `numactl --show | awk '/^physcpubind/ {printf \
     "+p%d +pemap %d",(NF-1),$2; for(i=3;i<=NF;++i){printf ",%d",$i}}'` ...
\end{verbatim}

\subsection{GPU Acceleration}

NAMD supports GPU-accelerated calculations for
NVIDIA GPUs using CUDA and AMD GPUs using HIP/ROCm.
The ``classic'' mode of running NAMD with GPU acceleration
offloads most of the force calculations to GPU devices
(GPU-offload mode) and runs the other calculations
(integration, rigid bond constraints, etc.) on the CPU.
Version 3.0 of NAMD introduces a GPU-resident mode
which calculates the entire dynamics calculations on GPU.

A port of NAMD's CUDA kernels to SYCL/oneAPI is in development
to support Intel GPUs (e.g., ALCF Aurora). A source code preview
release (version 2.15 alpha 2) providing SYCL support for
GPU-offload mode is available on the NAMD website.


\subsubsection{GPU-Offload Mode}

For GPU-offload mode, 
NAMD does not offload the entire calculation to the GPU, and performance
may therefore be limited by the CPU.  In general all available CPU cores
should be used, with CPU affinity set as described above.

Energy evaluation is slower than calculating forces alone, and the loss
is much greater in CUDA-accelerated builds.  Therefore you should set
outputEnergies to 100 or higher in the simulation config file.
% As this is a new feature you are encouraged to test all simulations
% before beginning production runs.
Forces evaluated on the GPU differ
slightly from a CPU-only calculation, an effect more visible in reported
scalar pressure values than in energies.

NAMD now has the entire force calculation offloaded to GPU
for conventional MD simulation options.
However,
not all advanced features are compatible with CUDA-accelerated NAMD builds,
in particular, any simulation option that requires modification
to the functional form of the nonbonded forces.
Note that QM/MM simulation is also disabled for CUDA-accelerated NAMD,
because the calculation is bottlenecked by
the QM calculation rather than the MM force calculation,
so can benefit from CUDA acceleration of the QM part when available.
Table~\ref{table:CUDA-accelerated} lists the parts of NAMD
that are accelerated with CUDA-capable GPUs,
and Table~\ref{table:CUDA-disabled} lists the advanced simulation
options that are disabled within a CUDA-accelerated NAMD build.

\begin{table}[htb]
\caption{NAMD GPU-offload mode: What is accelerated?}
\begin{center}
\begin{tabular}{c|c}
\textbf{Accelerated} & \textbf{Not Accelerated} \\ \hline
short-range nonbonded   & integration \\
PME reciprocal sum      & rigid bonds \\
bonded terms            & grid forces \\
implicit solvent        & collective variables \\
alchemical (FEP and TI) &
\end{tabular}
\end{center}
\label{table:CUDA-accelerated}
\end{table}

\begin{table}[htb]
\caption{NAMD GPU: What features are disabled?}
\begin{center}
\begin{tabular}{c|c}
\textbf{Disabled} & \textbf{Not Disabled} \\ \hline
Locally enhanced sampling
  & Memory optimized builds \\
Tabulated energies
  & Conformational free energy \\
Drude (nonbonded Thole)
  & Collective variables \\
Go forces
  & Grid forces \\
Pairwise interaction
  & Steering forces \\
Pressure profile
  & Almost everything else \\
QM/MM
  &
\end{tabular}
\end{center}
\label{table:CUDA-disabled}
\end{table}


\subsubsection{GPU-Resident Mode}

GPU-resident mode for MD simulation is a feature new to version 3.0.
Unlike GPU-offload mode that offloads the force calculation to
GPU devices while performing integration and rigid bond constraints
on the host CPU,
GPU-resident mode also performs integration and rigid bond constraints
on the GPU device and, most importantly, maintains the simulation
data on the GPU device between time steps.
By removing the performance bottleneck resulting from
host--device memory transfers and CPU kernel calculations
performed at every time step,
MD simulation performance on modern GPU hardware is more than doubled.

The new GPU-resident mode is also capable of scaling a simulation across
multiple GPUs within a single node as long as the GPUs have peer access
to directly read and write each others' local memories,
typically configured when all devices are mutually connected by
a high-speed networking fabric such as NVLink/NVSwitch for NVIDIA GPUs
or Infinity Fabric for AMD GPUs.
Representative configurations are found in
desktop workstations with an NVLinked pair of NVIDIA GPUs,
NVIDIA DGX computers, and the nodes of many supercomputers,
e.g., OLCF Frontier (AMD), OLCF Summit (NVIDIA),
ALCF Polaris (NVIDIA), and NERSC Perlmutter (NVIDIA).

GPU-resident mode for now exploits shared-memory parallelism,
which means that it is limited to multicore and netlrts-smp builds,
where the latter supports multiple replica GPU-resident simulations
with each replica running as a single process.

Unlike GPU-offload mode which generally benefits
from a larger number of CPU cores,
GPU-resident mode offers limited host-side work
requiring fewer CPU cores.
In fact, attempting to run too many CPU cores with GPU-resident mode
can slow down performance, due to the increased overhead of synchronizing
those cores without enough work to reasonably utilize them.
For GPU-resident mode, the number of CPU cores (PEs) to use
depends on the size of the system, the features being used,
the number of GPU devices used,
and the relative performance of the CPU cores compared to the GPU devices.
It is recommended to run benchmarks to determine the
optimal core count for your hardware.
The command \texttt{benchmarkTime} can be used with \texttt{outputTiming}
to easily benchmark a system from the command line without needing
to modify the config file. For example,
\begin{verbatim}
  ./namd3 +p4 +setcpuaffinity --outputTiming 500 --benchmarkTime 180 <configfile>
\end{verbatim}
will terminate the simulation after running for three minutes (180 seconds),
detected at the next output from \texttt{outputTiming}.

Since GPU-resident mode performs all calculation on the GPU device,
advanced features must generally be supported by porting a given
feature to the GPU.
Several features have been ported to GPU-resident mode
but many others still need to be ported.
Some of these advanced features are available for
multi-GPU scaling and others are single-GPU only.
There are also some features that are now provided as
GPU-resident-only high-performance alternatives to
already existing GPU-offload features.
Table~\ref{table:gpures-features} lists the features
available to GPU-resident mode, indicating support for multi-GPU scaling
and what methodologies are replaced, if any.

\begin{table}[htb]
\caption{NAMD GPU-resident mode: What features are supported?}
\begin{center}
\begin{tabular}{c|c|c}
\textbf{Feature} & \textbf{Multi-GPU} & \textbf{Replacing} \\ \hline
Essential dynamics                      & yes & \\
4-site water models (TIP4P and OPC)     & yes & \\
Alchemical (FEP and TI)                 & yes & \\
Multi-replica                           & yes & \\
Replica exchange solute scaling (REST2) & yes & \\
Harmonic restraints                     & no  & Fixed atoms \\
External electric field                 & no  & \\
Monte Carlo barostat                    & no  & Langevin piston \\
Group position restraints               & no  & Colvars distance restraints \\
Colvars                                 & yes & \\
TCL forces                              & yes &
\end{tabular}
\end{center}
\label{table:gpures-features}
\end{table}

Essential dynamics includes the standard ensembles (constant energy,
constant temperature with Langevin damping or stochastic velocity rescaling,
and constant pressure and temperature with Langevin piston)
together with rigid bond constraints, multiple time stepping,
and PME electrostatics.
Note that fixed atoms are not yet supported by GPU-resident,
with harmonic restraints recommended as a workaround until
support for fixed atoms is finished.
The Monte Carlo barostat offers a faster pressure control method
than Langevin piston, by avoiding calculation of the pressure virial
at every step.
Group position restraints is a NAMD-native GPU-resident implementation of
Colvars distance restraints, providing much higher performance than Colvars.
Both Colvars and TCL forces can be used with GPU-resident mode,
but their use might significantly impact performance since either one
requires host--device data transfer and CPU host calculations every step.
The impact to performance depends on what collective variables have
been defined and the number of atoms affected.
%Not all of the multi-GPU features listed above are
%correctly supported by GPUAtomMigration (discussed below).

Whether or not to use multi-GPU scaling for a simulation depends on
the size of the system and the capabilities of the GPU. 
For example, the 1M-atom STMV benchmark system gets reasonably
good scaling efficiency across an 8-GPU NVIDIA DGX-A100.
A reasonable rule of thumb seems to be around 100k atoms per GPU for
the Ampere series of GPUs and twice that per GPU for Hopper.

Multi-GPU scaling performance for GPU-resident mode
is significantly impacted by PME.
The issue is that due to the difficulty of scaling the 3D FFT calculations,
the long-range (gridded) part of PME is delegated to a single GPU,
and NAMD's default work decomposition scheme, to evenly distribute
patches to CPU cores and evenly distribute CPU cores to devices
will naturally overload the PME device.
The workaround is to use task-based parallelism to
restrict the amount of ``standard'' work to the PME device.
The approach exploits the existing load balancing performed by
NAMD during its startup by simply reducing the number of PEs
assigned to the PME device through the new ``+pmepes''
command line argument.
Note that good load balancing should maintain the same number of
PEs on the non-PME devices, which means that the overall
number of PEs set by ``+p'' will necessarily be reduced.
Setting this argument is best determined by benchmarking
the given system on the intended hardware platform,
which was done to determine
optimal settings for the 1M-atom STMV benchmark system
running on DGX-A100, using 8 PEs per device:
\begin{verbatim}
  ./namd3 +p8 +setcpuaffinity +devices 0 stmv.namd
  ./namd3 +p15 +pmepes 7 +setcpuaffinity +devices 0,1 stmv.namd
  ./namd3 +p29 +pmepes 5 +setcpuaffinity +devices 0,1,2,3 stmv.namd
  ./namd3 +p57 +pmepes 1 +setcpuaffinity +devices 0,1,2,3,4,5,6,7 stmv.namd
\end{verbatim}
Since performance for MD exhibits predominantly linear scaling
(up to reasonable size and resource utilization limits),
the ratios shown above for STMV can be applied as a starting
rule-of-thumb for other systems.

GPU-resident mode can also provide very fast simulation for smaller systems.
For example, the AMBER DHFR (23.6k atoms) benchmark,
using AMBER force field parameters with 9\;{\AA} cutoff, PME,
rigid bond constraints, and hydrogen mass repartitioning with 4\;fs time step,
can be simulated on A100 with over 1 microsecond/day performance.
When simulating smaller systems like DHFR,
performance is improved by using
\texttt{twoAwayZ on} to double the patch count,
producing a greater number of work units to schedule
across the GPU processing units.

Small systems will not achieve good scaling across multiple GPUs.
Instead, the most effective way to use multi-GPU architectures is to
simulate multi-copy ensembles.
Depending on the size of the system and the hardware capability,
GPU resources are often most efficiently used by running
multiple simulations per GPU,
in which performance can be measured as the aggregate number
of simulated nanoseconds per day achieved.
NVIDIA provides technologies MPS (Multi-Process Service)
and MIG (Multi-Instance GPU) that can facilitate running
multiple simultaneous NAMD jobs on a single GPU.


\subsubsection{GPU Hardware Requirements}

To benefit from GPU acceleration using NVIDIA GPU hardware
you will need a CUDA build of NAMD and a recent NVIDIA video card.
CUDA builds will not function
without a CUDA-capable GPU and a driver that supports CUDA 9.1.  If the
installed driver is too old NAMD will exit on startup with the error
``CUDA driver version is insufficient for CUDA runtime version.''
GPUs of compute capability $<$ 5.0 are no longer supported and are ignored.
GPUs with two or fewer SMs are ignored unless specifically
requested with +devices.

Finally, if NAMD was not statically linked against the CUDA runtime
then the libcudart.so file included with the binary (copied from
the version of CUDA it was built with) must be in a directory in your
LD\_LIBRARY\_PATH before any other libcudart.so libraries.  For example,
when running a multicore binary (recommended for a single machine):

\begin{verbatim}
  setenv LD_LIBRARY_PATH ".:$LD_LIBRARY_PATH"
  (or LD_LIBRARY_PATH=".:$LD_LIBRARY_PATH"; export LD_LIBRARY_PATH)
  ./namd3 +p8 +setcpuaffinity <configfile>
\end{verbatim}

NAMD can be built to run with GPU acceleration
on HIP-compatible AMD GPUs. Build instructions can be
found in the NAMD distribution notes.txt file.
For HIP builds, NAMD has been tested with ROCm 5.4.2 and 5.7.0,
and the HIP builds maintain feature parity with CUDA builds.

Each NAMD thread can use only one GPU.  Therefore you will need to run
at least one thread for each GPU you want to use.
Multiple threads in an SMP build of NAMD
can share a single GPU, usually with an increase in performance.  NAMD
will automatically distribute threads equally among the GPUs on a node.
Specific GPU device IDs can be requested via the +devices argument on
the namd3 command line, for example:

\begin{verbatim}
  ./namd3 +p8 +setcpuaffinity +devices 0,2 <configfile>
\end{verbatim}

Devices are shared by consecutive threads in a process, so in the
above example threads 0--3 will share device 0 and threads 4--7 will
share device 2.  Repeating a device will cause it to be assigned to
multiple master threads, which is allowed only for different processes
and is advised against in general but may be faster in certain cases.
When running on multiple nodes the +devices specification is applied to
each physical node separately and there is no way to provide a unique
list for each node.

When running a multi-node parallel job it is recommended to have one
process per device to maximize the number of communication threads.
If the job launch system enforces device segregation such that not all
devices are visible to each process then the +ignoresharing argument
must be used to disable the shared-device error message.

When running a multi-copy simulation with both multiple replicas and
multiple devices per physical node, 
the +devicesperreplica $<${\em n}$>$ argument
must be used to prevent each replica from binding all of the devices.
For example, for 2 replicas per 6-device host use +devicesperreplica 3.

While charmrun with ++local will preserve LD\_LIBRARY\_PATH, normal
charmrun does not.  You can use charmrun ++runscript to add the namd3
directory to LD\_LIBRARY\_PATH with the following executable runscript:

\begin{verbatim}
  #!/bin/csh
  setenv LD_LIBRARY_PATH "${1:h}:$LD_LIBRARY_PATH"
  $*
\end{verbatim}

For example:

\begin{verbatim}
  ./charmrun ++runscript ./runscript ++n 4 ./namd3 ++ppn 15 <configfile>
\end{verbatim}

An InfiniBand network is highly recommended when running GPU-accelerated
NAMD across multiple nodes.  You will need either a verbs NAMD binary
(available for download) or an MPI NAMD binary (must build Charm++ and
NAMD as described above) to make use of the InfiniBand network.  The use
of SMP binaries is also recommended when running on multiple nodes, with
one process per GPU and as many threads as available cores, reserving
one core per process for the communication thread.

The CUDA (NVIDIA's graphics processor programming platform) code in
NAMD is completely self-contained and does not use any of the CUDA
support features in Charm++.  When building NAMD with CUDA support
you should use the same Charm++ you would use for a non-CUDA build.
Do NOT add the cuda option to the Charm++ build command line.  The
only changes to the build process needed are to add --with-cuda for
GPU-offload support or --with-single-node-cuda for GPU-resident support
and possibly --cuda-prefix ... to the NAMD config command line.

NAMD can also be built with HIP/ROCm to support compatible AMD GPUs,
otherwise matching all features available with CUDA builds.
The build configuration options for HIP/ROCm are changed to
--with-hip for GPU-offload support or --with-single-node-hip for
GPU-resident support and --rocm-prefix ... to specify the library path.

For now, NAMD does not support all available features on GPU.
Some keywords have been introduced to give the user better control over
GPU calculation. These keywords are relevant only for GPU builds
and are ignored if the user is running a CPU-only build.

\subsubsection{Keywords}

\begin{itemize}
\setlength{\itemsep}{0.4cm}

\item
\NAMDCONFWDEF{GPUresident}{Run dynamics calculations entirely on GPU}%
{``on'' or ``off''}{off}{%
GPU-resident enabled builds also require setting the GPUresident keyword
in order to run dynamics calculations entirely on GPU. Without setting this 
keyword, GPU-enabled builds of NAMD will run in GPU-offload mode by default.
Replaces the deprecated \texttt{\icommand{CUDASOAintegrate}} keyword.
}

\item
\NAMDCONFWDEF{GPUAtomMigration}{Perform atom migration on the device}%
{``on'' or ``off''}{off}{%
An experimental optimization for GPU-resident mode simulation
that performs atom migration on the GPU device rather than the CPU host.
Using this can give faster performance, especially for smaller systems,
but this feature is still considered experimental.
For smaller systems, performance can be further improved by also 
setting ``twoAwayZ on'' to double the number of patches,
improving GPU utilization by
increasing the work units to be scheduled across the device.
With \texttt{GPUAtomMigration} enabled,
the maximum patch size is limited to 2048 atoms;
exceeding this limit will cause NAMD to terminate with an error message.
If patch sizes are close to this limit, then restarting the simulation
with ``twoAwayZ on'' will reduce the sizes by around one-half.
Replaces the deprecated \texttt{\icommand{DeviceMigration}} keyword.
}

\item
\NAMDCONFWDEF{GPUForceTable}{Always use force table interpolation for
nonbonded GPU kernel}{``on'' or ``off''}{on}{%
An experimental optimization for GPU-resident mode simulation.
Setting ``GPUForceTable off'' will use direct math calculation
for non-PME steps (i.e., for multiple time stepping when the
error function is \emph{not} being computed).
For recent GPUs direct calculation is faster for certain cases
than force table lookup and interpolation.
Not all nonbonded kernel variants are supported, for example,
the standard van der Waals switching function is implemented,
but the ``vdwForceSwitching'' function is not.
Similarly, alchemical free energy calculations
with FEP and TI are not supported.
Replaces the deprecated \texttt{\icommand{CUDAForceTable}} keyword.
}

\item
\NAMDCONFWDEF{bondedGPU}{0 to 255}{Integer value between 0 and 255}{255}{%
NAMD provides GPU kernels for calculating
six different bonded force terms.
This parameter is irrelevant to GPU-resident mode,
since all forces are calculated on the GPU. 
For GPU-offload mode, the bondedGPU parameter acts as a bit mask
that can disable particular kernels. 
Any partial sum of the following values can
be used to enable only the specified bonded terms:
\begin{itemize}
\item bonds $=$ 1
\item angles $=$ 2
\item dihedrals $=$ 4
\item impropers $=$ 8
\item exclusions $=$ 16
\item crossterms $=$ 32
\end{itemize}
Replaces the deprecated \texttt{\icommand{bondedCUDA}} keyword.
}

\item
\NAMDCONFWDEF{usePMEGPU}{Offload entire PME reciprocal sum to GPU?}%
{``on" or ``off"}{on}{%
For GPU-resident mode, the entire PME reciprocal sum is 
automatically run on a single GPU, 
so it is not necessary to set this parameter. 
For GPU-offload mode, whether or not usePMEGPU is enabled depends on how
NAMD is run.  It is automatically enabled when running on a single node
and is disabled when running across multiple nodes. 
Replaces the deprecated \texttt{\icommand{usePMECUDA}} keyword.
}

\item
\NAMDCONFWDEF{PMEoffload}{Offload PME gridding/ungridding procedures to GPU?}%
{``on" or ``off"}{off}{%
This parameter is irrelevant for GPU-resident mode. 
For GPU-offload mode, 
the gridding and ungridding procedures for calculating the PME
reciprocal sum is offloaded to GPUs,
with the FFT calculation still performed by CPUs.
PMEoffload is enabled by default only for PMEinterpOrder $>$ 4.

For huge systems (10 million atoms and above)
where the parallel FFT limits performance,
it is desirable to use PMEoffload in conjunction with
increased order interpolation and increased grid spacing,
in order to decrease the overall communication latency
by decreasing the overall grid size by a factor of 8
while maintaining the same accuracy for the calculation.

\vspace{-0.25em}
\textbf{Exemplary use:}
\texttt{%
\vspace{-1em}
\small{%
\begin{tabbing}
PME on \\
PMEinterpOrder 8 \\
PMEgridSpacing 2.0 \\
PMEoffload on   ;\# enabled by default for these PME settings
\end{tabbing}
} % \small
} % \texttt
} % \NAMDCONFWDEF


\end{itemize}

\subsection{Xeon and Zen4 Acceleration}

NAMD supports CPU-only optimizations for processors that implement
particular AVX-512 vector instruction sets (avx512f and avx512dq).
These instruction sets have been available for several years with
Intel Xeon and more recently with AMD Zen4 processors.
A vectorized kernel replaces the standard nonbonded force kernel,
reimplementing the CUDA tiles algorithm for AVX-512 using the
\emph{Intrinsics for Intel AVX-512 Instructions}.
Use of these optimizations is not compatible with GPU-accelerated builds.

The downloads page offers a multicore AVX-512 binary built
using GCC 11.2.1 that is compatible with either Xeon or Zen4.
Otherwise, build target \texttt{Linux-AVX512-icx}
(using a recent Intel oneAPI toolkit) is recommended for Xeon
and build target \texttt{Linux-AVX512-g++}
(using GCC version 11 or later) is recommended for Zen4.
The AVX-512 tiles optimization is enabled by default.
Since use of this optimized kernel is not compatible with
all features, NAMD will automatically disable its use
when an incompatibility is detected,
falling back to the standard CPU code path.
There is also a config file parameter to explicitly disable
use of the tiles optimization.
\begin{itemize}
\item
\NAMDCONFWDEF{useAVXTiles}{Use the AVX-512 tiles optimization}%
{``on'' or ``off''}{on}{%
This parameter affects only AVX-512 builds.
The AVX-512 tiles optimization for calculating nonbonded forces
is enabled by default but can be disabled by setting this
parameter ``off.''
}
\end{itemize}


\subsection{Xeon Phi Acceleration}

NAMD supports offloading calculations to Intel Xeon Phi coprocessors.
This feature is new and should be considered experimental.  Observed
speedups are around a factor of two, but parallel scaling is degraded.

The Xeon Phi coprocessor is supported in NAMD similar to CUDA GPUs.
Binaries are not provided, so you will need to build from source code
(see ``Compiling NAMD'' below) specifying --with-mic to the config script.
As with CUDA, multicore or ibverbs-smp builds are strongly recommended.
A recent Intel compiler is obviously required to compile for Xeon Phi.

\subsection{Memory Usage}

NAMD has traditionally used less than 100MB of memory even for systems
of 100,000 atoms.  With the reintroduction of pairlists in NAMD 2.5,
however, memory usage for a 100,000 atom system with a 12A cutoff can
approach 300MB, and will grow with the cube of the cutoff.  This extra
memory is distributed across processors during a parallel run, but a
single workstation may run out of physical memory with a large system.

To avoid this, NAMD now provides a pairlistMinProcs config file option
that specifies the minimum number of processors that a run must use
before pairlists will be enabled (on fewer processors small local
pairlists are generated and recycled rather than being saved, the
default is ``pairlistMinProcs 1'').  This is a per-simulation rather than
a compile time option because memory usage is molecule-dependent.

Additional information on reducing memory usage may be found at
\url{http://www.ks.uiuc.edu/Research/namd/wiki/index.cgi?NamdMemoryReduction}

\subsection{Improving Parallel Scaling}

While NAMD is designed to be a scalable program, particularly for
simulations of 100,000 atoms or more, at some point adding additional
processors to a simulation will provide little or no extra performance.
If you are lucky enough to have access to a parallel machine you should
measure NAMD's parallel speedup for a variety of processor counts when
running your particular simulation.  The easiest and most accurate way
to do this is to look at the ``Benchmark time:'' lines that are printed
after 20 and 25 cycles (usually less than 500 steps).  You can monitor
performance during the entire simulation by adding ``outputTiming {\em steps}''
to your configuration file, but be careful to look at the ``wall time''
rather than ``CPU time'' fields on the ``TIMING:'' output lines produced.
For an external measure of performance, you should run simulations of
both 25 and 50 cycles (see the stepspercycle parameter) and base your
estimate on the additional time needed for the longer simulation in
order to exclude startup costs and allow for initial load balancing.

Multicore builds scale well within a single node, but may benefit from
setting CPU affinity using the +setcpuaffinity +pemap $<$map$>$ +commap $<$map$>$
options described in CPU Affinity above.  Experimentation is needed.

We provide standard (UDP), TCP, and ibverbs (InfiniBand) precompiled
binaries for Linux clusters.  The TCP version may be faster on some
networks but the UDP version now performs well on gigabit ethernet.
The ibverbs version should be used on any cluster with InfiniBand,
and for any other high-speed network you should compile an MPI version.

SMP builds generally do not scale as well across nodes as single-threaded
non-SMP builds because the communication thread is both a bottleneck and
occupies a core that could otherwise be used for computation.  As such
they should only be used to reduce memory consumption or if for scaling
reasons you are not using all of the cores on a node anyway, and you
should run benchmarks to determine the optimal configuration.

Extremely short cycle lengths (less than 10 steps) will limit parallel
scaling, since the atom migration at the end of each cycle sends many
more messages than a normal force evaluation.  Increasing margin from
0 to 1 while doubling stepspercycle and pairlistspercycle may help,
but it is important to benchmark.  The pairlist distance will adjust
automatically, and one pairlist per ten steps is a good ratio.

NAMD should scale very well when the number of patches (multiply the
dimensions of the patch grid) is larger or rougly the same as the
number of processors.  If this is not the case, it may be possible
to improve scaling by adding ``twoAwayX yes'' to the config file,
which roughly doubles the number of patches.  (Similar options
twoAwayY and twoAwayZ also exist, and may be used in combination,
but this greatly increases the number of compute objects.  twoAwayX
has the unique advantage of also improving the scalability of PME.)
\index{twoAwayX} \index{twoAwayY} \index{twoAwayZ}

Additional performance tuning suggestions and options are described
at \url{http://www.ks.uiuc.edu/Research/namd/wiki/?NamdPerformanceTuning}

